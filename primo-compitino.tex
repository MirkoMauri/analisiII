\documentclass[a4paper,10pt]{book}
\usepackage[utf8x]{inputenc}
\usepackage{amssymb, amsmath}
\setcounter{chapter}{1}
\newcommand{\ubar}{\underbar}

\author{Studenti vari}
\title{Schema per il primo compitino}

\begin{document}

\maketitle

\section{Continuità e differenziabilità}
\subsection{Definizioni principali}
\paragraph{Derivata direzionale}
$$D_{\underbar{v}} \vec{f}(\underbar{x}_0) = \lim_{h \to 0}{\dfrac{\vec{f}(\underbar{x}_0+h\underbar{v})-\vec{f}(\underbar{x}_0)}{h}}$$
ed è utile sapere che in basi canoniche:
$$D_{\vec{v}} \vec{f}(\underbar{x}_0) = d\vec{f}(\underbar{x}_0)\cdot\vec{v}$$

\paragraph{Differenziale}
Il differenziale è un'applicazione lineare che soddisfa la relazione:
$$f(\vec{x_0}+\vec{h}) - f(\vec{x_0}) - df(\vec{x_0})\cdot\vec{h} = o(|h|)$$
ovvero (supponendo $f:\mathbb{R}^2\rightarrow\mathbb{R}$)
$$ \lim_{|(h, k)| \to 0} \dfrac{f(\vec{x_0}+(h, k)) - f(\vec{x_0}) - df(\vec{x_0})(h, k)}{\sqrt{h^2+k^2}} = 0 $$
Come vedremo sotto, per funzioni $f: \mathbb{R}^2 \rightarrow \mathbb{R}$
$$df(\underbar{x}_0) = \nabla \cdot f(\underbar{x}_0)$$
\paragraph{Matrice Jacobiana}
$$J_f = \begin{bmatrix} \dfrac{\partial f_1}{\partial x_1} & \cdots & \dfrac{\partial f_1}{\partial x_n} \\ \vdots & \ddots & \vdots \\ \dfrac{\partial f_m}{\partial x_1} & \cdots & \dfrac{\partial f_m}{\partial x_n}  \end{bmatrix}$$
In basi canoniche:
$$ d\vec{f}(\underbar{x}_0)= J_f(\underbar{x}_0) $$
\subsection{Come risolvere un esercizio}
\paragraph{Continuità in un punto $(x_0, y_0)$}

Bisogna ovviamente dimostrare che, $$\lim_{\ubar{x}\rightarrow\ubar{x}_0}\ubar{f}(\ubar{x}) = \ubar{f}(\ubar{x}_0)$$
Puoi passare in coordinate polari, ricordando che questa trasformazione non è biunivoca (vedi sez. "Ricorda").

\paragraph{Differenziabilità in un punto $(x_0, y_0)$}
Per questo esercizio, in generale, si può procedere in due modi:
\begin{itemize}
 \item Applicando la definizione, dunque calcolando direttamente $\nabla f(x_0, y_0)$, e verificando che vada a 0 il limite
 $$\lim_{|(h, k)| \to 0} \dfrac{f(\vec{x_0}+(h, k)) - f(\vec{x_0}) - \nabla f(\vec{x_0})(h, k)}{\sqrt{h^2+k^2}} = 0 $$
 \item Applicando il teorema del differenziale totale: bisogna dimostrare che, in un intorno di $(x_0, y_0)$ $f$ ammetta derivate parziali, e che queste siano continue in $(x_0, y_0)$. A questo punto $df(x_0, y_0) = \nabla f(x_0, y_0)$
\end{itemize}

\subsection{Ricorda}
\paragraph{Teorema di Schwarz}
Se la funzione $f(x,y)$ ammette derivate parziali miste in $(x_0, y_0)$, e queste sono ivi continue, allora in questo punto esse sono uguali ($\partial_{xy}f(x_0, y_0) = \partial_{yx}f(x_0, y_0)$).
\subsection{Trucchetti}
È spesso utile passare in coordinate polari: $$ x = \rho cos(\theta)$$ $$ y = \rho sin(\theta) $$
Ricorda che
$$ \lim_{|(x, y)| \to 0} f(x, y) = \lim_{\rho \to 0} f(\rho, \theta) $$
\textbf{ma questa conversione non è biunivoca!}
In generale questo non è un problema, ma se ad esempio usi questo passaggio per verificare la continuità, devi verificare che questo valga anche per la semiretta $\{(x, y) : x\geq 0, y=0\}$


\section{Massimi e minimi}
\subsection{Definizioni principali}

\paragraph{Hessiana}
Si definisce matrice Hessiana della funzione f:
$$  H_{f}(\mathbf{x})_{ij} = \frac{\partial^2 f(\mathbf{x})}{\partial x_i\, \partial x_j} $$
$$ H(f) = \begin{bmatrix} \dfrac{\partial^2 f}{\partial x_1^2} & \dfrac{\partial^2 f}{\partial x_1\,\partial x_2} & \cdots & \dfrac{\partial^2 f}{\partial x_1\,\partial x_n} \\ \\ \dfrac{\partial^2 f}{\partial x_2\,\partial x_1} & \dfrac{\partial^2 f}{\partial x_2^2} & \cdots & \dfrac{\partial^2 f}{\partial x_2\,\partial x_n} \\ \\ \vdots & \vdots & \ddots & \vdots \\ \\ \dfrac{\partial^2 f}{\partial x_n\,\partial x_1} & \dfrac{\partial^2 f}{\partial x_n\,\partial x_2} & \cdots & \dfrac{\partial^2 f}{\partial x_n^2} \end{bmatrix}$$
Nota: se tutte le derivate seconde di f sono continue in una regione $\Omega$, allora l'hessiana di f è una matrice simmetrica in ogni punto di $\Omega$. (v. Teorema di Schwarz)

\paragraph{Minori principali di Nord Ovest}
I minori principali di Nord Ovest della matrice $A$ sono le sottomatrici quadrate della $A$ ottenute prendendo le prime $k$ righe dallŽalto e le prime $k$ colonne da sinistra di $A$.
Data la matrice:
\begin{displaymath}
A=\left(\begin{array}{cccc}
a_{1,1} & a_{1,2} & \dots &a_{1,n} \\

a_{2,1} & a_{2,2} & \dots &a_{2,n} \\

\vdots & \vdots & \ddots & \vdots \\

a_{n,1} & a_{n,2} & \dots &a_{n,n} \\
\end{array}\right)
\end{displaymath}
il minori principale di Nord Ovest di dimensione $k$ è la sottomatrice:
\begin{displaymath}
A_k=\left(\begin{array}{ccc}
a_{1,1} &  \dots &a_{1,k} \\


\vdots & \ddots & \vdots \\

a_{k,1} & \dots &a_{k,k} \\
\end{array}\right)
\end{displaymath}

\subsection{Come risolvere un esercizio}
\begin{itemize}
\item Calcola le derivate parziali $\frac{\partial f}{\partial x}$, $\frac{\partial f}{\partial y}$, e risolvere il sistema (all'interno di D)
\begin{displaymath}
\left\{ \begin{array}{ll}
\frac{\partial f}{\partial x}=0\\
\\
\frac{\partial f}{\partial y}=0\\
\end{array} \right.
\end{displaymath}
i punti $\boldsymbol{x_0}=(x_0,y_0)$ che risolvono tale sistema sono detti punti stazionari.

\item Usa la matrice Hessiana per determinare il tipo di punto stazionario.
\begin{itemize}
 \item Calcola gli autovalori della matrice [$A = H(f(\boldsymbol{x_0}))$; $det(A-\lambda I)$]
 \begin{itemize}
 \item Se sono tutti positivi, $A$ e'definita positiva.
 \item Se sono tutti negativi, $A$ e'definita negativa.
 \item Se tutti gli autovalori della matrice $A$ sono maggiori di $0$ e ne esiste almeno uno uguale a $0$, allora la forma quadratica è semidefinita positiva, ma non definita positiva.
 \item Se tutti gli autovalori della matrice $A$ sono minori di $0$ e ne esiste almeno uno uguale a $0$, allora la forma quadratica è semidefinita negativa, ma non definita negativa.
 \item Se sono in parte positivi, in parte negativi $A$ è indefinita.
 \end{itemize}
 \item Alternativamente, siano $A_k$ i minori principali di Nord-Ovest di dimensione $k$.
Sia $\Delta_k=det(A_k)\ \forall k=1,\dots,n$ (cioe'il determinante del minore principale di NO di $dim=k$) allora:
 \begin{itemize}
  \item Se $\Delta_k > 0 \forall k$ $A$ è definita positiva.
  \item Se $(-1)^k\cdot\Delta_k > 0 \forall k$ $A$  è definita negativa.
  \item Se $\Delta_k \geq 0 \forall k$ $A$ è semidefinita positiva. 
  \item Se $(-1)^k\cdot\Delta_k \geq 0 \forall k$ $A$  è semidefinita negativa. 
  \item Se non vale nessuna delle condizioni precedenti la matrice è indefinita 
 \end{itemize}
 \item a questo punto sappiamo che:
 \begin{itemize}

 \item Se $A$ è definita positiva il punto è minimo
 \item Se $A$ è definita negativa il punto è massimo
 \item Se $A$ è indefinita il punto è di sella
 \item Se $A$ è semidefinita il caso è ambiguo.
 \end{itemize}
 
\end{itemize}
\item Come risolvere il caso ambiguo?
\begin{itemize}
\item se $f(\boldsymbol {x_0})=0$, ove $\boldsymbol {x_0}$ è punto stazionario, allora si procede allo studio del segno della funzione:
\begin{itemize}
\item se $\exists U_{\boldsymbol {x_0}}\ni\boldsymbol {x_0}$, intorno di $\boldsymbol {x_0}$, tale che $\forall \boldsymbol{x}\in U_{\boldsymbol {x_0}}\\f(\boldsymbol{x})<0$, allora $\boldsymbol {x_0}$ è punto di massimo forte.
\item se $\exists U_{\boldsymbol {x_0}}\ni\boldsymbol {x_0}$, intorno di $\boldsymbol {x_0}$, tale che $\forall \boldsymbol{x}\in U_{\boldsymbol {x_0}}\\f(\boldsymbol{x})\leq0$, allora $\boldsymbol {x_0}$ è punto di massimo debole.
\item se $\exists U_{\boldsymbol {x_0}}\ni\boldsymbol {x_0}$, intorno di $\boldsymbol {x_0}$, tale che $\forall \boldsymbol{x}\in U_{\boldsymbol {x_0}}\\f(\boldsymbol{x})>0$, allora $\boldsymbol {x_0}$ è punto di minimo forte.
\item se $\exists U_{\boldsymbol {x_0}}\ni\boldsymbol {x_0}$, intorno di $\boldsymbol {x_0}$, tale che $\forall \boldsymbol{x}\in U_{\boldsymbol {x_0}}\\f(\boldsymbol{x})\geq0$, allora $\boldsymbol {x_0}$ è punto di minimo debole.
\end{itemize}
Infatti le condizioni precedenti verificano nei vari casi la definizione stessa di punto di massimo o di minimo di una funzione.
\item se $f:\mathbb{R}^2\to\mathbb{R}$ si può considerare la restrizione di $f$ alle curve conosciute del piano (rette, parabole, cubiche..). Se risulta che la funzione ristretta a una di queste curve non ammette estremante ma solo punto stazionario (es: punto di flesso), sicuramente $f$ non avrà ne un massimo ne un minimo in $\boldsymbol {x_0}$, che sarà quindi punto di sella.
\item ultima spiaggia: provare con la definizione di massimo e minimo a dimostrare che il punto è un massimo, un minimo, o che non può essere ne l'uno ne l'altro ed è quindi un punto di sella.
\end{itemize}
\item Possono esistere punti di minimo o massimo sul bordo:
\begin{itemize}
 \item Restringi la funzione al bordo.
 \item Calcola la derivata e valuta i punti stazionari (nota: se la funzione era $f: \mathbb{R}^2 \rightarrow \mathbb{R}$, la restrizione sarà del tipo $h: \mathbb{R} \rightarrow \mathbb{R}$, dove $h(x) = f(x, g(x))$.
 \item In caso la derivata non sia banale, approssimare il valore del punto stazionario, dando almeno un intervallo entro il quale è compreso.
\end{itemize}
\end{itemize}

\subsection{Ricorda}
\subsection{Trucchetti}

\section{Funzioni implicite}
\subsection{Definizioni principali}
\paragraph{Funzione implicita}
Sia $f: \mathbb{R}^2 \rightarrow \mathbb{R}$.
Sia $\phi(x)$ tale che $f(x, \phi(x)) = 0$ $\forall x \in \mathbb{R}$. $\phi(x)$ si dice funzione definita implicitamente da $f(x, y)$ rispetto a $y$.

\paragraph{Esistenza e unicità}
\subparagraph{In grande}
\subparagraph{In piccolo}

\subsection{Come risolvere un esercizio}
\begin{itemize}
 \item Valuta se valgono o no le ipotesi del teorema di esistenza e unicità "in grande"
 \begin{itemize}
  \item Siano $y_1$ e $y_2$ estremi del dominio, o di uno specifico intervallo.
  \item Calcola $a = \lim_{y \to y_1}{f(x, y)}$ e $b = \lim_{y \to y_2}{f(x, y)}$.
  \item Se $a < 0$ e $b > 0$, $\exists \phi(x)$.
  \item Altrimenti, prova a cercare un altro intervallo in cui questo è vero.
 \end{itemize}
 \item Prova a studiare $\phi'(x) = -\displaystyle\frac{\partial_x f}{\partial_y f}$
\end{itemize}

\subsection{Ricorda}
\subsection{Trucchetti}

\section{Integrali}
\subsection{Nozioni principali}
\paragraph{Misura di un insieme}
Sia $\Omega \subset \mathbb{R}^n$ un insieme limitato. $\Omega$ è misurabile $\leftrightarrow m(\partial\Omega) = 0$, e in particolar modo questo è vero quando $\partial\Omega = \cup G_f$ (funzioni continue).
In questo caso
$$m(\Omega) = \int \mathbb{I}_\Omega$$
\paragraph{Insieme semplice}
Un insieme semplice (rispetto a $y$) è un insieme E del tipo:
$$ E = \{(x, y) \in \mathbb{R}^2 : a<x<b, g(x)<y<h(x) \} $$
Gli insiemi semplici sono particolarmente utili per semplificare gli esercizi di integrazione.

\paragraph{Diffeomorfismi tra aperti}
(se $\Omega$ non è aperto, considero $\Omega^\circ = \bar{\Omega}\setminus\partial\Omega$)\\
$$\int \mathbb{I}_\Omega = \int \mathbb{I}_{\Omega^\circ} + \int \mathbb{I}_{\bar{\Omega}\setminus\partial\Omega} $$
perché $\Omega\setminus\Omega^\circ \in \partial\Omega$. Siccome $\Omega$ è misurabile, $\Omega^\circ$ è misurable ($\partial\Omega = \partial\Omega^\circ, m(\partial\Omega) = 0$, ed inoltre $m(\Omega\setminus\Omega^\circ) \leq m(\partial\Omega) = 0$).
Da cui
$$\int \mathbb{I}_\Omega = \int \mathbb{I}_{\Omega^\circ} $$
Poi, sia $\Phi: A \subset \mathbb{R}^n \rightarrow \Omega^\circ$, se $det(J_\Phi) \neq 0$ e $\Phi$ biunivoca ?<testo incomprensibile di edo>? Allora $\Phi$ è un diffeomorfismo globale se $\Phi \in \mathbb{C}^1(\Omega^\circ)$, e
$$\int_{\Omega^\circ} \mathbb{I} = \int_{\Phi(\Omega^\circ)} \mathbb{I} $$

\paragraph{Teorema di Fubini (riduzione)}
Sia $f(x, y)$ integrabile in $[a, b)\times[c,d)$ tale che $\forall x_0 \in [a,b), f(x_0, y) \in \Re[c,d)$, allora:
$$\int_R f(x, y)dxdy = \int^b_a dx\int^d_c f(x, y) dy$$

\subsection{Ricorda}
\begin{itemize}
 \item Se $G_f$ è grafico di una funzione, $m(G_f) = 0$.
\end{itemize}
\paragraph{Cambio di coordinate (diffeomorfismi)}
Se per risolvere il tuo integrale devi effettuare un cambio di coordinate, devi procedere con cautela.

Sia ad esempio $$(\rho, \theta) = \psi(x, y) = \left(\begin{matrix}\sqrt{x^2+y^2} \\ atan(y/x)\end{matrix}\right)^t$$
e sia $J_\psi$ la matrice jacobiana associata.
Allora, essendo $D=\psi(\Omega)$,\\ $\vec{x} = (x, y) \in \Omega$ e $\vec{p} = (\rho, \theta) \in D$.
$$ \int_\Omega f(\vec{x}) d\vec{x} = \int_{\psi(\Omega)} |det(J_\psi)|\cdot f(\psi(\vec{x})) d\psi(x) = \int_D |det(J_\psi)|\cdot f(\vec{p})d\vec{p} $$

\subsection{Come risolvere un esercizio}
\paragraph{Notazione:}
$\Omega$ insieme entro il quale integrare, $f:\mathbb{R}^2\rightarrow\mathbb{R}$ funzione integranda.
Inoltre $m(\Omega) := m_{\mathrm{PJ}}(\Omega)$
\begin{itemize}
 \item Controlla che $\Omega$ sia misurabile, $\mathcal{E}f$ integrabile (soprattutto: o limitata, o monotona, o gen. continua). In caso negativo fermati.
 \item Prega che le primitive siano semplici da trovare.
 \item 1: Puoi cercare un insieme semplice $D$, e trasformare $\Omega$ in $D$ tramite un diffeomorfismo.
 \begin{itemize}
  \item (vedi le note sopra su cambi di coordinate e prosegui)
 \end{itemize}
 \item 2: Puoi integrare sull'insieme $\Omega$ direttamente
 \begin{itemize}
  \item Cerca di applicare il teorema di Fubini per ricondurti al calcolo di variabili in una dimensione
 \end{itemize}
 \item Nel caso di integrali impropri:
 \begin{itemize}
  \item 1: Di prima specie: sia $f$ limitata, da integrare su $\Omega$ illimitato. Verifica che:
  \begin{itemize}
   \item $|\mathcal{E}f||_{B_R} \in \Re(B_R), \forall R$, ovvero:
   \begin{itemize}
    \item $B_R$ misurabile $\forall R$
    \item $B_R \cap \Omega \in B_R $ misurabile $\forall R$
    \item $f \in C^0(B_R)$ ($\rightarrow |\mathcal{E}f||_{B_R\cap\Omega}$ è generalmente continua, limitata, quindi $|\mathcal{E}f| \in \Re[B_R]$)
   \end{itemize}
   \item $\exists$ finito $$\lim_{R \to +\infty} \int_{B_R} |\mathcal{E}f| $$
   \item Allora: $$\int_\Omega f = \lim_{R \to +\infty} \int_{B_R} |\mathcal{E}f| $$
  \end{itemize}
  \item 2: Di seconda specie, $f$ illimitata, $\Omega$ limitato. Verifica che:
  \begin{itemize}
   \item $E_1 \in E_2 \in \cdots \in E_N \in \Omega$, tutti misurabili (secondo P-J).
   \item $f|_{E_J} \in \Re[E_j]$
   \item $m(\Omega\setminus E_j) \to 0$ (con $j \to +\infty$)
   \item Allora: $$\int_\Omega f = \lim_{J \to +\infty} \int_{E_J} |\mathcal{E}f|$$
  \end{itemize}
 \end{itemize}
\end{itemize}

\subsection{Trucchetti}
Ricorda che, data $f: \Omega \rightarrow \mathbb{R}$:
 $$\int_\Omega f = \int_{\mathbb{R}^n} \mathcal{E}f = \int \mathcal{E}f_+ - \int \mathcal{E}f_- $$

\section{Altro}
\subsection{Formulette}
\paragraph{Trigonometria}
\subparagraph{Formule parametriche}
$$sin(\alpha) = \frac{2t}{1+t^2}$$
$$cos(\alpha) = \frac{1-t^2}{1+t^2}$$
$$tan(\alpha) = \frac{2t}{1-t^2}$$
ove $$t = tan\left(\frac{\alpha}{2}\right)$$
\subsection{Nozioni utili}
\paragraph{Geometria analitica}
\subparagraph{Piano tangente al grafico di una funzione}
$$z = f(x_0, y_0) + \partial_x f(x_0, y_0) (x-x_0) + \partial_y f(x_0, y_0) (y - y_0) $$
\end{document}


