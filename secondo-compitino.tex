\documentclass[a4paper,12pt]{article}
\usepackage[utf8x]{inputenc}
\usepackage{amssymb, amsmath, amsthm, mathrsfs}
\newcommand{\ubar}{\underbar}
\newcommand{\cont}{\mathscr{C}}
\newcommand{\Rint}{\mathcal{R}}
%\newtheorem{d}{Definizione}
\usepackage{fullpage}

\newtheorem{teo}{Teorema}
\newtheorem{defi}{Definizione}
\newtheorem{cor}{Corollario}
\begin{document}
\begin{titlepage}
\title{Dispensa per il secondo compitino di Analisi II}
\author{Studenti vari}
% Remove command to get current date 
\maketitle
\end{titlepage}

\begin{titlepage}
\tableofcontents
\end{titlepage}

\section{Successioni e Serie di Funzioni}
\subsection{Definizioni}
\begin{defi}
Sia $\{ f_n\} $  una successione di funzioni a valori reali o complessi definite su un medesimo $D \subseteq \mathbb{R}$.
 $\{ f_n\} $ \textbf{converge puntualmente} o semplicemente su $ E \subseteq D $ se $\forall x \in E$, $\{f_n(x)\}$ converge in $\mathbb{R}$ (o nel caso complesso $\mathbb{C}$). Chiamiamo funzione limite puntuale la funzione definita su $E$ nel modo seguente: 
 $$f(x):=\lim_{n\to+\infty} f_n(x) \qquad \qquad \forall x \in E $$

Similmente, se $\forall x \in E \subseteq D$ la serie numerica $\sum_{n=1}^{\infty} f_n(x)$ converge in $\mathbb{R}$ (o $\mathbb{C}$)  allora diremo che la serie  $\sum_{n=1}^{\infty} f_n$ converge puntualmente in E. La funzione
\begin{displaymath}
s(x):= \sum_{n=1}^{\infty} f_n(x) \qquad \qquad \forall x \in E
\end{displaymath}
si dirà somma puntuale della serie $\sum_{n=1}^{\infty} f_n$
\end{defi}


\subparagraph*{Osservazione}
Utilizzando un'altra scrittura, la definizione di convergenza puntuale diviene:
 $$\forall \varepsilon>0\qquad \forall x \in E\qquad\exists N=N(\varepsilon, x): \forall n \geq N\qquad |f_n(x) - f(x)| < \varepsilon$$ 
\begin{defi}
Sia $\{ f_n\} $ come sopra. Essa \textbf{ converge uniformente} su $E \subseteq \mathbb{R} $ alla funzione limite $f$ se:
 
$$ \lim_{n\to+\infty} \displaystyle \sup_{x \in E} |f_n\left(x\right)-f\left(x\right)| = 0$$


Similmente diciamo che la serie $\sum_{n=1}^{\infty} f_n$ converge uniformemente su $E \subseteq \mathbb{R} $ alla funzione limite $s(x)$ se e solo se

$$ \lim_{n\to+\infty}  \sup_{x \in E} \displaystyle \left|\sum_{n=1}^{\infty} f_n(x)-s(x)\right| = 0 $$

\end{defi}
\subparagraph*{Osservazione}
 
Utilizzando un'altra scrittura, la definizione di convergenza uniforme diviene:
 $$\forall \varepsilon>0\qquad \exists N=N(\varepsilon):\ \forall n \geq N\qquad |f_n(x) - f(x)| < \varepsilon\qquad \forall x\in E$$ 

\subsection{Criteri di convergenza}

\subsubsection{Successioni}
\begin{teo}[Condizione di Cauchy]
Sia $\{f_n\}$ una successione di funzioni a valori reali (o complessi). Allora $\{f_n\}$ converge uniformemente in $E$ se e solo se $\forall \varepsilon>0\ \ \exists N=N(\varepsilon):\ \forall n,m \geq N\ \forall x\in E$ si ha che 
$$|f_n(x) - f_m(x)| < \varepsilon$$.
\end{teo}
\subparagraph*{Osservazione}
La convergenza uniforme di una successione è quindi condizione \textbf{necessaria e sufficiente} per verificare il criterio di Cauchy.
\begin{teo}
Sia ${f_n}$ una successioni di funzioni definite su $K$ compatto, $f_n \in \cont(K)$; sia $f_n$ convergente puntualmente a $f$,   $f \in \cont(K)$ . Se
\begin{displaymath}
f_1 \geq f_2 \geq f_3 \geq \dots
\end{displaymath}

allora $f_n$ converge a $f$ uniformemente.
\end{teo}
\subsubsection{Serie}

\begin{teo}[Criterio di Weierstrass]
Sia $\{f_n\}$ una successione di funzioni su $E$. Sia $$M_n:= \displaystyle\sup_{x\in E}|f_n(x)|$$ e $$\sum_{n=1}^{+\infty}M_n<+\infty$$
allora $\displaystyle\sum_{n=1}^{+\infty}f_n$ converge uniformemente.
\end{teo}
\subsection{Convergenza Uniforme e Continuità}
\begin{teo}[Successioni di funzioni continue]
Sia $\{f_n\}$ una successione di funzioni continue definite in $E$ uniformemente convergenti a $f$. Allora $f$ è continua.
\end{teo}
\subsection{Convergenza Uniforme e Integrabilità}
\begin{teo}
Sia ${f_n}$ una successione di funzioni R-integrabili in $[a,b]\subseteq \mathbb{R}$, uniformemente convergente a $f$ in $[a,b]$. Allora $f\in \Rint([a,b])$ e
$$\lim_{n\to\infty}\int_{a}^{b} f_n=\int_{a}^{b}f$$
\end{teo}
\subsection{Convergenza Uniforme e Differenziabilità}
\begin{teo}
Sia $f_n$ una successione di funzioni differenziabili in  $[a,b]\subseteq \mathbb{R}$. Supponiamo che esista un $x_{0} \in [a,b] $ per il quale la successione numerica ${f_n(x_0)}$ converga e che ${f^{'}(x)}$ converga uniformemente in $[a,b]$. Allora ${f_n}$ converge uniformemente in $[a,b]$ a una funzione $f$ e
$$\lim_{n\to\infty} f^{'}_n (x)=f^{'}(x) \qquad \qquad \forall x \in [a,b]$$
\end{teo}

\subsection{Serie di potenze}
\subsubsection{Definizioni}
Una serie di potenze è una serie di funzioni della forma
$$\label{seriepotenza} \displaystyle \sum_{n=0}^\infty c_n\left(x-x_0\right)^n$$
con $x_0 \in \mathbb{R}$ fissato, ${c_n}$ è successione di numeri reali e $x$ una variabile reale. 
E' sempre possibile ricondursi al caso $x_0 = 0$ con una opportuna traslazione. L'insieme di convergenza di \ref{seriepotenza} è l'insieme degli $x \in \mathbb{R}$ tali che \ref{seriepotenza} converge. L'insieme di convergenza di una serie di potenze è sempre un \textbf{intervallo.}
\paragraph{Teorema della convergenza semplice}
Data la serie di potenze 
$$ \displaystyle \sum_{n=0}^\infty c_n\left(x\right)^n$$
poniamo $$ \alpha:=\limsup_{n\to \infty} \sqrt[n]{|c_n x^n|} \ e \ R:=\frac{1}{\alpha}$$
(se $\alpha = +\infty$, allora $R=0$ e se $\alpha=0$, allora $R=+\infty$)
La serie di potenze converge se $|x|<R$ e non converge se $|x|>R$.
R è il raggio di convergenza della serie.
\paragraph{Teorema della convergenza uniforme delle serie di potenza}
Supponiamo che $ \displaystyle \sum_{n=0}^\infty c_n\left(x\right)^n$ converga per $|x|<r$. Poniamo:
$$ f(x):= \displaystyle \sum_{n=0}^\infty c_n\left(x\right)^n \ \ \forall t \in (-r,r) \forall \varepsilon > 0 $$
Allora $$ \displaystyle \sum_{n=0}^\infty c_n\left(x\right)^n$$ converge uniformente a $f(x)$ in $[-r+\varepsilon,r-\varepsilon]$ Inoltre: $f \in \cont^\infty((-r,r))$ $$f^{(k)}(x) = \displaystyle \sum_{n=0}^\infty n(n-1)\dots(n-k+1)c_n x^{n-k} \ \ \forall x \in (-r,r)$$  $$f^{(k)}(0) = k!\ c_k \ \ k=0,1,2,3 \dots $$ 

\paragraph{Serie di Taylor}
La serie:
$\displaystyle \sum_{n=0}^\infty \frac{f^{n}(0)}{n!} x^n$ si chiama serie di Taylor di $f$. Condizione necessaria affinchè la serie di Taylor di $f$ converga in  $x$ è che $$\lim_{n \to \infty}\displaystyle \left[ f(x) - \displaystyle  \sum_{k=0}^\infty \frac{f^{k}(0)}{k!}x^k \right] = 0 $$


\subsection{Come risolvere gli esercizi}

\subsubsection{Successioni}
Solitamente gli esercizi sulle successioni di funzioni richiedono, data la successione ${f_n}$, di studiarne la convergenza (puntuale e uniforme) in un insieme $E\subseteq \mathbb{R}$ e, nel caso $f_n$ non converga uniformemente in $E$, di trovarne gli intervalli di convergenza. Vediamo come procedere.
\paragraph*{Dominio}
Data la successione ${f_n}$ la prima cosa da fare è studiarne il dominio. Questo aiuterà poi a restringere l'insieme di convergenza uniforme. Ovviamente poiché le $f_n$ dipendono anche dalla $n$ si potranno avere diversi domini $Dn$ in dipendenza da $n$. Ma poiché il dominio deve valere per tutte le funzioni $f_n$, $\forall n\in\mathbb{N}$, si avrà
$$D=\displaystyle\bigcap_{n=1}^{\infty} D_n$$
\paragraph*{Convergenza puntuale}
Per studiare la convergenza puntuale è sufficiente eseguire il limite
$$\lim_{n\to\infty} f_n(x)$$
Eseguendo questo limite si potrà eventualmente notare come ${f_n}$ converga puntualmente solo per alcuni valori di $x\in D$, restringendo così l'intervallo di convergenza a un insieme $B\subseteq D$ in cui $f_n$ converge puntualmente alla funzione $f$.
\paragraph*{Casi costanti}
Può succedere che per alcuni valori particolari di $x\in D\subseteq E$ si trovino dei casi costanti rispetto a $n$. Siano essi ad esempio $x_i \in D$, per i quali quindi si ha che $f_n(x_i)=k \quad \forall n$  ove $k\in \mathbb{R}$. In tal caso allora 
$$\lim_{n\to\infty} f_n(x_i)=\lim_{n\to\infty} k = k $$
allora ovviamente $f_n$ converge puntualmente a $k$ in $x_i$.
\paragraph*{Convergenza uniforme}
Per studiare la convergenza uniforme si procede applicando la definizione di convergenza uniforme sull'insieme $B$ di convergenza puntuale, ossia si procede verificando la seguente uguaglianza
$$\lim_{n\to\infty} \sup_{x\in B} |f_n(x)- f(x)|=0$$.
Il calcolo dell'estremo superiore può non essere facile. Tuttavia se $B$ è chiuso, e $f_n, f \in\cont(B)$,   per il teorema di Weierstrass la funzione $\varphi:=|f_n-f|$ avrà un minimo e in particolare un massimo in $B$. Ovviamente questo massimo coincide con l'estremo superiore della medesima funzione. Sarà quindi sufficiente trovare il massimo di tale funzione, che potrà essere facilmente trovato studiando il segno della derivata prima.
\paragraph*{Restrizione dell'insieme di convergenza uniforme}
Può accadere che $f_n$ non converga uniformemente a $f$ in $B$, insieme di convergenza puntuale, ossia che 
$$\lim_{n\to\infty} \sup_{x\in B} |f_n(x)- f(x)|\neq 0$$
Sia ora $x_m$ tale che $\sup_{x\in B} |f_n(x)- f(x)|=\sup_{x\in B} \varphi(x)=\varphi (x_m)$. Allora è possibile che restringendo anche di poco l'insieme $B$ a un insieme $\overline{B}\subseteq B$, questo $x_m$ non sia più contenuto in $\overline{B}$, e si abbia un nuovo punto di massimo $\overline{x_m}$ di $\overline{B}$ per cui 
$$ \sup_{x\in \overline{B}} |f_n(x)- f(x)|=\varphi(\overline{x_m})\to 0 $$
Allora si avrà che $f_n$ converge uniformemente in $\underline{B}$.
\paragraph*{Esempio}
Facciamo un piccolo esempio pratico:
Sia $$f_n(x)= nx(1-x^2)^n \qquad \qquad \forall x\in I:=[0,1]$$
Studiamo prima di tutto la convergenza puntuale.
Si ha che $f_n(0)=f_n(1)=0 \\ \forall n$, mentre se $0<x<1$ allora 
$$\lim_{n\to\infty}nx(1-x^2)^n=0$$
poiché prevale l'esponenziale $(1-x^2)^n:=k^n$ ove $k<1$.
Quindi $f_n $ converge puntualmente alla funzione $f$ tale che $f(x)=0 \ \forall x \in[0,1]$.
Studiamo la la convergenza uniforme: dobbiamo calcolare la quantità
$$\sup_{x\in[0,1]}|f_n(x)-f(x)|=\sup_{x\in[0,1]}f_n(x)$$ 
poiché $f(x)=0 \ \forall x \in [0,1]$ e $f_n(x)\geq0 \ \forall x \in [0,1]$.


Studiamo la derivata prima $f^{'}_n$ per trovare il massimo in I.
\begin{displaymath}
f^{'}_n(x)=n(1-x^2)^{n-1}(1-(1+2n)x^2)\geq 0 \quad \wedge \quad x\in I
\end{displaymath}
$$ \Updownarrow$$
$$0\leq x\leq \frac{1}{\sqrt{1+2n}}$$
posto $$x_m=\frac{1}{\sqrt{1+2n}}$$ avremo
$$\sup_{x\in[0,1]}f_n(x)=f(x_m)\to + \infty$$
Quindi $f_n$ non convergerà uniformemente in $I$.
Tuttavia possiamo notare come $x_m\to 0$ per $n\to + \infty$, e come $f_n(x)$ sia decrescente in $[x_m,1]$.
Poiché $x_m\to 0$, $\forall \delta>0, \ \exists n_0: \ \forall n>n_0 \ x_m<\delta$, quindi nell'insieme $[\delta,1]:=\overline{I}\subseteq I$ il massimo di $f_n$ non sarà più in corrispondenza di $x_m$. Inoltre poiché $f_n(x)$ è decrescente in $[x_m,1]$, a maggior ragione $f_n(x)$ decresce in $\overline{I}$.

Quindi si avrà che 
$$\sup_{x\in[\delta,1]}f_n(x)=f_n(\delta)\rightarrow_{n\to\infty} 0  $$.
Quindi $f_n$ convergerà uniformemente in $\overline{I}=[\delta,1]$
 
\subsubsection{Serie}
\subparagraph*{Osservazione}
Per risolvere gli esercizi sulle serie bisogna comunque tenere conto che quanto visto per le successioni continua a valere ancora per le serie. Infatti presa una serie $\sum_{n=1}^{\infty}f_n$ essa può essere vista come 
$$\sum_{n=1}^{\infty}f_n=\lim_{k\to\infty}F_k$$
ove $F_n$ è la successione delle somme parziali
$$F_k:=\sum_{n=1}^{k}f_n$$
\subparagraph*{}
Solitamente gli esercizi sulle serie di funzioni richiedono, data la serie $\sum_{n=1}^{\infty}f_n$, di studiarne la convergenza (puntuale e uniforme) in un insieme $E\subseteq \mathbb{R}$ e, nel caso $\sum_{n=1}^{\infty}f_n$ non converga uniformemente in $E$, di trovarne gli intervalli di convergenza. Vediamo come procedere.
\paragraph*{Dominio}
La prima cosa da fare è studiarne il dominio della serie. Questo aiuterà poi a restringere l'insieme di convergenza uniforme. Ovviamente poiché varrà lo stesso ragionamento fatto per le successioni per le $f_n$; si avrà quindi il dominio $D$
$$D=\displaystyle\bigcap_{n=1}^{\infty} D_n$$
\paragraph*{Convergenza puntuale}
Per studiare la convergenza puntuale è sufficiente eseguire il limite
$$\lim_{k\to\infty} F_k(x)$$
ove $F_k$ successione delle somme parziali, ossia per studiare la convergenza puntuale è sufficiente lo studio della convergenza della serie numerica $\sum_{n=1}^{\infty}f_n(x) \qquad \forall x \in D$.
Effettuando questo studio si potrà eventualmente notare come $\sum_{n=1}^{\infty}f_n$ converga puntualmente solo per alcuni valori di $x\in D$, restringendo così l'intervallo di convergenza a un insieme $B\subseteq D$ in cui $\sum_{n=1}^{\infty}f_n$ converge puntualmente alla funzione $s$.
\paragraph*{Criteri di convergenza per serie numeriche}
Poiché lo studio della convergenza puntuale richiede lo studio di serie numeriche è utile ricordare alcuni criteri di convergenza delle serie numeriche.
\subparagraph*{Convergenza assoluta}
\begin{defi}
Si dice che una serie numerica $\sum_{n=1}^{\infty}a_n$ converge assolutamente se converge la serie
$$\sum_{n=1}^{\infty}|a_n|$$
\end{defi}
\begin{teo}
Se una serie numerica converge assolutamente allora converge.
\end{teo}
\subparagraph*{Criterio della radice}
\begin{teo}
Sia  $\sum_{n=1}^{\infty}a_n$ una serie numerica definitivamente positiva. Esista $\alpha$ tale che
$$\alpha=\lim_{n\to\infty}\sqrt[n]{a_n}$$
allora:
\begin{itemize}
\item Se $0\leq\alpha<1$, allora la serie numerica converge
\item Se $\alpha>1 $ allora la serie diverge.
\end{itemize}
\end{teo}
\subparagraph*{Criterio della rapporto}
\begin{teo}
Sia  $\sum_{n=1}^{\infty}a_n$ una serie numerica definitivamente positiva. Esista $\alpha$ tale che
$$\alpha=\lim_{n\to\infty}\frac{a_{n+1}}{a_n}$$
allora:
\begin{itemize}
\item Se $0\leq\alpha<1$, allora la serie numerica converge
\item Se $\alpha>1 $ allora la serie diverge.
\end{itemize}
\end{teo}
\subparagraph*{Osservazione}
In entrambi i casi nulla si può dire se $\alpha=1$
\subparagraph*{Criterio di Leibniz}
\begin{teo}
Sia $\sum_{n=1}^{\infty} (-1)^n a_n$ serie numerica tale che:
\begin{itemize}
\item  $a_n > 0$ per ogni $n$
\item $a_n$ successione non crescente
\item $\lim_{n\to\infty} a_n=0$
\end{itemize}
Allora la serie converge.
\end{teo}
\begin{cor}
Se $a_n$ verifica le ipotesi del teorema di Leibniz, sia $\sum_{n=1}^{\infty} (-1)^n a_n$ convergente al valore $s$. Allora si ha la seguente formula per l'errore
$$E_n=\left|s- \sum (-1)^n a_n  \right|\leq a_{n+1}$$
\end{cor}
\paragraph*{Convergenza Uniforme}
Per studiare la convergenza uniforme può essere vantaggioso procedere applicando prima di tutto il Criterio di Wierstrass. In tal modo si riduce lo studio della convergenza uniforme allo studio della convergenza di una serie numerica (vedi Criterio di Weierstrass), risolvibile per mezzo dei criteri prima enunciati. In alternativa si può applicare il Criterio di Cauchy, o, in alternativa, cercare di verificare la convergenza dalla definizione, così come si faceva per le successioni. 

La verifica attraverso la definizione risulta molto utile per serie a segni alterni del tipo
$$\sum_{n=1}^{\infty}(-1)^n f_n$$
convergenti puntualmente a una funzione $s$ per il criterio di Leibniz. Infatti applicando il corollario del criterio e la definizione di convergenza assoluta si ha che:
$$\sup_{x\in B}\left|s- \sum (-1)^n f_n(x)  \right|\leq f_{n+1}(x)$$
Ma poiché è applicabile il criterio di Leibniz $f_{n+1}(x)\to0$ per $n\to\infty$ e, di conseguenza, la serie converge uniformemente poiché
$$\lim_{n\to\infty}\sup_{x\in B}\left|s- \sum (-1)^n f_n(x)  \right|\leq \lim_{n\to\infty} f_{n+1}(x)=0$$
\subsubsection{Serie di potenze}


\section{Contrazioni in spazi metrici}
\subsection{Definizioni}
Sia $(X, d)$ uno spazio metrico, sia $\phi : X\rightarrow X$.
\paragraph{Contrazione}
Si dice che $\phi$ è una contrazione di $X$ in sé quando:
$$\exists c < 1:\ \ d(\phi(x_1), \phi(x_2)) \leq cd(x_1, x_2)\qquad\qquad\forall x_1, x_2 \in X$$
\paragraph{Punto fisso}
Si dice che $\bar{x}\in X$ è punto fisso per $\phi$ se:
$$\phi(\bar{x})=\bar{x}$$

\subsection{Lipschitzianità}
Sia $f:\Omega\rightarrow\mathbb{R}$ ove $\Omega\subset\mathbb{R}^n$, aperto.
\paragraph{Lipschitzianità semplice}
Si dice $f\left(x\right)$ lipschitziana se $\exists L>0$ t.c
$$ |f\left(x\right)- f\left(y\right)| \leq L|x-y|\qquad\qquad\forall x,y \in \Omega $$

\paragraph{Lipschitzianità uniforme}
Si dice che $f(t, v)$ è lipschitziana rispetto a $v$, uniformemente rispetto a $t$, su $\Omega$ se $\exists L>0$ t.c.
$$|f(t, v) - f(t, w)| \leq L|v-w|\qquad\qquad\forall (t,v), (t, w) \in \Omega$$
In altre parole, fissata la variabile $t$, $f_t(v)$ è lipschitziana.

\paragraph{Lipschitzianità locale}
Si dice che $f(t, v)$ è localmente lipschitziana rispetto a $v$, uniformemente rispetto a $t$, se
$$\exists U((t, v), r) : f(t, v)|_U \in \text{Lip}_t(v)\qquad\qquad\forall(t, v)\in \Omega$$
(esiste un intorno $U$ di $(t, v)$ tale che la funzione ristretta a quell'intorno sia lipschitziana rispetto a $v$, uniformemente rispetto a $t$).

\subsection{Teoremi}
\paragraph{Teorema di Banach-Caccioppoli}
Sia $(X, d)$ uno spazio metrico, sia $\phi : X\rightarrow X$ una contrazione.
Allora $\phi$ ha un unico punto fisso.


\section{Equazioni differenziali}

\subsection{Teoria}
\paragraph{Il problema di Cauchy}
Siano $f:\Omega \subseteq \mathbb{R}^2 \rightarrow \mathbb{R}$ e $ \left(t_0,y_0\right) \in \Omega$. Il problema di Cauchy (PC) è il sistema
$$\label{PC}\begin{cases} y'=f\left(t,y\right) \\ y\left(t_0\right)=y_0 \end{cases}$$

Una soluzione del problema di Cauchy è una coppia $\left( \phi , I \right)$ costituita da una funzione $\phi: I \rightarrow \mathbb{R}$ differenziabile in $I$ tale che:
\begin{itemize}
\item $t_0 \in I\qquad\text{e}\qquad \phi\left(t_0\right)=y_0$
\item $ \left(t,\phi\left(t\right)\right) \in \Omega \qquad \forall t \in I$
\item $ \phi^{'}\left(t \right) = f\left(t,\phi\left(t\right)\right)\qquad \forall t \in I$
\end{itemize}

Se $\left(\phi,I\right)$ risolve il problema di Cauchy, allora è una soluzione continua dell'equazione di Volterra:
$$ y\left(t\right) = y_0 + \displaystyle\int^t_{t_0} f\left(s,y\left(s\right)\right) ds $$

\paragraph{Teorema di esistenza e unicità in piccolo o locale}
Se $f \in \cont \left(\Omega\right)$ allora PC ha almeno una soluzione. \\
Se $f \in \cont \left(\Omega\right) \cap \text{Lip}^{\text{loc}}_x\left(\Omega\right)$, allora PC ha un'unica soluzione in un opportuno intorno di $t_0$.\\
Una funzione di classe $\cont^n$ con $ n \geq 1 $ è localmente lipschitziana. 

\paragraph{Teorema di esistenza e unicità in grande (o globale)}
Siano $-\infty < \tau_1 < \tau_2 < +\infty$ e $\S:=\left[\tau_1,\tau_2\right]\times\mathbb{R}$. Sia $f \in C\left(\Omega\right) \cap \text{Lip}^{\text{loc}}_x\left(\Omega\right)$, con crescita al più lineare in x, cioè
$$\exists a,b: |f \left(t,x\right)|\leq a + b|x|$$
Allora l'unica soluzione del (PC)
$$\begin{cases} x'=f\left(t,x\right) \\ x\left(t_0\right)=x_0 \end{cases}$$
(ove $\left(t_0,x_0\right)$ è un punto qualunque di $S$), è definita in $\left[\tau_1,\tau_2\right]$.

\paragraph{Teorema di prolungamento delle soluzioni}

Siano $$f \in C\left(\Omega\right) \cap \text{Lip}^{\text{loc}}_x\left(\Omega\right)\qquad \text{e} \qquad\left(t_0,x_0\right) \in \Omega$$
Sia $I=(t_{min},t_{max})$ un intervallo tale che la coppia $\left(\phi,I\right)$ sia la soluzione massimale di (PC). Sia inoltre $K\subset \Omega$ compatto.

Allora il grafico di $\phi$ esce definitivamente da $K$ per $t\rightarrow t^+_{min}$ e $t\rightarrow t^-_{max}$, con 

\paragraph{Matrice wronskiana}
Siano $\phi_1,...,\phi_n$ funzioni a valori su $\mathbb{R}^n$ definite in $I\subset \mathbb{R}$.
Si chiama \textbf{matrice wronskiana} di $\phi_1,...,\phi_n$ la matrice:
$$W(t):=[\phi_1(t)\ ...\ \phi_n(t)]$$
Si chiama altresì \textbf{wronskiano} di $\phi_1,...,\phi_n$:
$$w(t):=\text{det } W(t)$$
\subparagraph*{Osservazione} l'annullarsi del wronskiano è condizione necessaria (ma non sufficiente) per la dipendenza lineare di $\phi_1,...,\phi_n$. 
Condizione necessaria e sufficiente affinchè $n$ soluzioni su $I$ di una medesima equazione lineare omogenea siano linearmente indipendenti è che:
$$ w(t)\neq0 \qquad\qquad \forall t \in I$$


\subsection{Principali tipi di equazioni differenziali}
\subsubsection{Sistemi lineari omogenei}
Un sistema lineare omogeneo (LO) di equazioni differenziali è un sistema della forma:
$$\underbar{x}' = A(t)\underbar{x}$$ 
ove $$A(t)\in M_n(\mathbb{R})\qquad \qquad \forall t\in I\subset R$$ e $\underbar{x} = (x_1,x_2,x_3 \dots x_n)^t$
\paragraph{Osservazioni}
\begin{itemize}
 \item Lo spazio delle soluzioni di LO è uno spazio vettoriale di dimensione $n$.
 \item Il PC associato a LO ha una e una sola soluzione definita su tutto $I$.
 \item $A(t)\underbar{x}$ ha crescita al più lineare.
 \item Se $\phi_1,...,\phi_n$ sono soluzioni di LO, queste sono linearmente indipendenti se e solo se $w(t)\neq0$ $\forall t \in I$.
\end{itemize}


\subsubsection{Sistemi lineari non omogenei}
Un sistema lineare non omogeneo (LNO) di equazioni differenziali è un sistema della forma:
$$\underbar{x}' = A(t)\underbar{x} + \underbar{b}(t)$$
\paragraph{Osservazioni}
\begin{itemize}
 \item Lo spazio delle soluzioni di (LNO) non è uno spazio vettoriale, se $\underbar{b}\neq\underbar{0}$.
 \item Sia $\Psi^*$ un soluzione di (LNO). Tutte e sole le soluzioni di (LNO) sono allora del tipo
       $$ \Psi(t) = \phi(t) + \Psi^*(t)\qquad \qquad \forall t \in I$$
       dove $\phi$ è una soluzione qualunque dell'(LO) associato ($\underbar{b}=\underbar{0}$).
 \item Se $A(t) , \underbar{b}\in \cont(I)$, allora $ \forall t_0 \in I$ il (PC)  ha una e una sola soluzione.
 \item Se $\phi_1,...,\phi_n$ sono soluzioni di (LO), queste sono linearmente indipendenti se e solo se $w(t)\neq0$ $\forall t \in I$.
\end{itemize}


\paragraph{Integrale generale}
L'integrale generale è una soluzione di una equazione differenziale.\\
Ad esempio, l'integrale generale, o soluzione generale, di un'equazione differenziale lineare non omogenea è data dalla somma tra una funzione soluzione dell'omogenea associata e una soluzione dell'equazione non omogenea. La soluzione particolare dell'equazione non omogenea è anche detta \textbf{integrale particolare}. Parlare di integrare un'equazione differenziale è equivalente a parlare di risolvere un'equazione differenziale.

\subsubsection{Sistemi lineari omogeni a coefficienti costanti}
Un sistema lineare omogeneo a coefficiente costante (LOCC) è un sistema
$$\underbar{x}' = A\underbar{x}$$
dove $A \in M_n(\mathbb{R})$ e $x,x'\in\mathbb{R}^n$. Possiamo esprimere le soluzioni di questo sistema mediante l'esponenziale della matrice $tA$, ove $t$ è uno scalare.
L'esponenziale di matrici gode delle seguenti proprietà:
\begin{itemize}
\item $e^{(s+t)A} = e^{sA}e^{tA} \ \ \forall s,t \in \mathbb{R} $
\item $\det e^{tA}=e^{t\cdot\text{tr}(A)} \ \forall t \in \mathbb{R} $
\item $\dfrac{d}{dt} e^{tA} = A e^{tA} = e^{tA}A$
\end{itemize}

\paragraph{Teorema} Per ogni (vettore colonna) $\underbar{c} \in \mathbb{R}^n $, la funzione $$ \underbar{$\varphi$}(t) = e^{tA} \underbar{c} $$è soluzione dell'eq (LOCC) su tutto $\mathbb{R}$.

\subsubsection{Sistemi lineari non omogenei a coefficienti costanti}
Un sistema lineare non omogeneo (a coefficienti costanti) di equazioni differenziali è un sistema del tipo
$$\underbar{x}'=A\underbar{x}+\underbar{b}(t)$$
Ove $A\in M_n(\mathbb{R})$ e $b$ continua, allora, per ogni punto $t_0 \in \mathbb{R}$:
$$\Psi^*(t)= \int_{t_0}^t e^{(t-s)A} \underbar{b}(s)ds$$
è una soluzione particolare dell'equazione sopra.
Per orttenere una soluzione più specifica, ad esempio ponendo una condizione iniziale quale
$$\underbar{x}(t_0)=\underbar{x}_0$$
Allora si ha che
$$\phi(t)=e^{(t-t_0)A}\underbar{x}_0+\Psi^*(t)$$
ovvero si somma alla soluzione generale del sistema omogeneo associato la soluzione particolare trovata in precedenza.
Vedi anche il metodo di variazione delle costanti arbitrarie.

\subsubsection{Equazioni di ordine n a coefficienti costanti}
Sia $\lambda \in \mathbb{C}$: la funzione $e^\lambda t$ è soluzione di (ELOCC) se e solo se $\lambda$ è uno zero del polinomio caratteristico di (ELOCC) definito da: $$ p(\lambda) := \lambda^n + a_1\lambda^{(n-1)}+ \dots +a_{n-1}\lambda+a_n$$
\paragraph*{Osservazione}
Se  $\lambda_1,\dots,\lambda_n$ sono complessi a due a due distinti, allora 
$$ e^{\lambda_1 t}, \dots ,e^{\lambda_n t}$$ sono funzioni linearmente indipendenti. 



\subsection{Risoluzione esercizi}
In generale, risolvere (integrare) un'equazione differenziale consiste principalmente nel riconoscere un tipo di equazione, ed applicare procedimenti noti. Vediamone qualcuno.

\subsubsection{Grafici delle soluzioni}
todo marco
\subsubsection{Metodo di variazione delle costanti arbitrarie}
Questo metodo (alternativo, e un pochino più semplice) viene utilizzato se conosciamo un sistema fondamentale di soluzioni di un (LO), e cerchiamo almeno una soluzione di un (LNO).
Siano $\phi_1,...,\phi_n$ soluzioni di un (LO) $y'= A(t)y$, e sia $W(t)$ la corrispondente matrice wronskiana.
Ricerchiamo ora una soluzione di (LNO) della forma
$$\Psi^*(t)=W(t)\underbar{c}(t)$$
ove le nostre incognite sono le varie componenti della funzione $\underbar{c}(t)$
Ricordando che vale la formula:
$$\frac{dW(t)}{dt} = A(t)W(t)$$
(per verifica diretta) si trova che, chiamando $V(t) = W^-1(t)$, e imponendo che $$\underbar{c}'(t) = V(t)\underbar{b}(t)$$
si trova l'integrale particolare cercato:
$$\Psi^*(t)= W(t)\int_{t_0}^t V(t)\underbar{b}(s) ds$$

\subsubsection{Equazioni lineari}
Con un'equazione di forma:
$$y'(t) = P(t)y+Q(t)$$
Allora
$$y(t)=e^{- \displaystyle\int P(t)dt}\left(k + \int Q(t)e^{-\displaystyle\int P(t)dt}dt\right)$$
\subsubsection{Equazione di Bernoulli}
Con un'equazione di forma:
$$y'(t) = P(t)y+Q(t)$$
con $\alpha \neq 0, 1$. Allora, si impone:
$$ z(t)=y(t)^{1-\alpha} $$
si risolve l'equazione lineare in $z$: $$z'(t)=(1-\alpha)P(t)z(t)+Q(t)$$ e, trovata $z(t)$ con la formula sopra, si ricava $y(t)$.

\subsubsection{Equazione di Riccati}
$$y'(t) = P(t)y^2+Q(t)y+R(t)$$
Conoscendo una soluzione particolare $\Psi(t)$, allora applico la sostituzione
$$y(t)=\Psi(t)+\dfrac{1}{z(t)}$$
e con un po' di passaggi si dimostra che alla fine
$$z'(t)= -\left[2\Psi(t)P(t)+Q(t)\right]z(t)-P(t)$$
che è lineare.
\subsubsection{Variabili separabili}
Se invece
$$y'(t) = f(t)g(y)$$
Allora ovviamente si risolve ricordando che
$$\int\dfrac{dy}{g(y)}=\int f(t)dt + c$$
\subsubsection{Un'altro tipo di equazioni omogenee}
$$y'(t) = f\left(\dfrac{y(t)}{t}\right)$$
Allora si sostituisce
$$z=\dfrac{y}{t}$$
da cui $$z'(t) = \dfrac{dz}{dt} = \dfrac{f(z)-z}{t}$$
e dunque
$$\int\dfrac{dz}{f(z)-z}=\int \dfrac{dt}{t} + c$$
sicché alla fine:
$$\int\dfrac{dz}{f(z)-z} = \log|t| + c$$

Risostituendo e risolvendo rispetto a $y$ si ottiene la soluzione cercata.

\subsubsection{Equazioni lineari di ordine $n$}
Sia
$$y^{(n)} + a_1(t)y^{(n-1)} + a_2y^{(n-2)} + ... + a_n(t)y = 0$$
ove le funzioni $a_j \in C(I)$
Allora il sistema è equivalente al sistema:
$$\underbar{Y}' = A(t)\underbar{Y}$$
ove $\underbar{Y}=(y, y', y^{(2)}, ..., y^{(n)})$ (ogni componente è trattata come componente a sé stante, e non esplicitamente come derivata) e
$$A(t) =
\begin{bmatrix}
0 & 1 & 0 & ... & 0\\
0 & 0 & 1 & ... & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & ... & 1 \\
-a_n(t) & -a_{n-1}(t) & -a_{n-2}(t) & ... & -a_{1}(t) \\
\end{bmatrix}
$$

che si risolve come un (LO) qualunque.

\subsubsection{Oscillatori armonici}
\paragraph{Oscillazioni libere}
Data un'equazione di forma
$$x''+\omega^2 x=0$$
la soluzione è del tipo
$$\psi(t) = c_1 e^{i\omega t} + c_2 e^{-i\omega t}$$
oppure
$$\psi(t) = c'_1\sin(\omega t) + c'_2\cos(\omega t)$$
a seconda della base scelta.

\paragraph{Risonanza??}
Invece, data un'equazione di forma
$$x''-\omega^2 x=0$$
questa ha soluzioni reali, più precisamente del tipo
$$\psi(t) = c_1 e^{\omega t} + c_2 e^{-\omega t}$$

\paragraph{Oscillazioni smorzate}
Nel terzo caso, abbiamo un'equazione del tipo
$$x''-2x +x=0$$
la cui soluzione è del tipo
$$\phi(t) = c_1e^t +c_2 t + e^t$$
(check)

\paragraph{Casi non omogenei}
In un caso del tipo
$$x''+\omega^2 x=b(t)$$
bisogna sommare una soluzione particolare. Si scopre facilmente che tutte e sole le soluzioni sono fatte
$$\phi(t) = c'_1\sin(\omega t) + c'_2\cos(\omega t) + \frac{1}{\omega^2}t$$

o equivalentemente scegliendo una base complessa, la soluzione particolare da sommare è $\psi^*(t) = \frac{1}{\omega^2}t$.

\subsection{Equazioni differenziali esatte}

Un'equazione differenziale esatta è una forma del tipo

\begin{equation}\label{eq-diff-esatte}
y'(t)=\frac{P(t, y)}{Q(t,y)} 
\end{equation}

bisogna trovare una 1-forma differenziale $\omega$ esatta. Chiamando $F$ la funzione di cui $\omega$ è differenziale, si trova che:

$$\omega = Q(t,y)dy - P(t,y)dt = \frac{\partial F}{\partial y}dy+\frac{\partial F}{\partial t}dt = dF$$
che è vero se e solo se
$$\frac{\partial F}{\partial t}=-P(t,y)$$
$$\frac{\partial F}{\partial y}=Q(t,y)$$
a questo punto l'equazione
$$F(t, y(t)) = \text{costante}$$
definisce in maniera implicita tutte e sole le soluzioni dell'equazione differenziale (\ref{eq-diff-esatte}).

Nota: condizione necessaria per cui (\ref{eq-diff-esatte}) abbia soluzione è che
$$\frac{\partial P}{\partial y}=-\frac{\partial Q}{\partial t}$$

\subsubsection{Altre cose da trovare}
A volte capita che bisogni trovare una funzione $g(x,y)$ tale che $g\cdot\omega$ (ove il punto indica la moltiplicazione semplice) sia ancora una forma esatta. In questo caso:
$$P\frac{\partial g}{\partial y} + Q\frac{\partial g}{\partial t} = -g\left(\frac{\partial P}{\partial y} + \frac{\partial Q}{\partial t}\right)$$
che è molto complicato.
Se $g=g(t)$, allora si può riscrivere come:
$$\frac{\partial_tg}{g} = -\frac{1}{Q}(\partial_yP+\partial_tQ)$$
e se $g=g(y)$
$$\frac{\partial_yg}{g} = -\frac{1}{P}(\partial_yP+\partial_tQ)$$

\paragraph{Esempio}
Sia ad esempio
$$y'=-\frac{y+2xy^3}{x+3x^2y^2}$$
In questo caso
$$\partial_xQ=1+6xy^2\qquad \partial_yP=-1-6xy^2$$
Se
$$\frac{\partial F}{\partial x}=-P = y+2xy^3$$
allora
$$F(x,y(x)) = \int (y+2xy^3)dx = yx+y^2x^2 + g(y)$$
e derivando $F$ rispetto a $y$, questa deve soddisfare la relazione
$$\frac{\partial F}{\partial y}=Q(x,y)\qquad(=x+3x^2y^2)$$
Dunque
$$x+3x^2y^2+g'(y)=x+3x^2y^2$$
Quindi in particolare $g'(x) = 0$ e dunque $g(x)$ è una costante. L'integrale generale è dunque
$$F(x, y(x)) = yx+x^2 + c$$

\section{Curve in $\mathbb{R}^n$}
\subsection{Definizioni principali}
\paragraph{Curva, sostegno}
Una curva in $\mathbb{R}^n$ è una funzione continua $\gamma:[a,b]\rightarrow\mathbb{R}^n$.
Il suo sostegno è l'immagine di $[a,b]$ tramite $\gamma$. Viene anche detto ``parametrizzazione''.
\paragraph{Velocità vettoriale}
Sia $\gamma$ una curva in $\mathbb{R}^n$. Allora la velocità vettoriale $\underbar{v}(t)$ è definita come
$$\underbar{v}(t) := \lim_{t\to t_0} \dfrac{\gamma(t)-\gamma(t_0)}{t-t_0}$$

\paragraph{Regolarità}
Una curva $\gamma:[a,b]\rightarrow\mathbb{R}^n$ si dice regolare se è di classe $C^1$. Si dice invece regolare a tratti se esiste una partizione $\{x_j\}$ di $[a,b]$ tale che $\gamma|_{[x_j, x_{j+1}]}$ è regolare.
\paragraph{Equivalenza}
Due curve regolari $\gamma :[a,b]\rightarrow\mathbb{R}^n$ e $\delta :[c,d]\rightarrow\mathbb{R}^n$ si dicono equivalenti se esiste una funzione 
$f:[a,b]\rightarrow[c,d], f \in C^1([a,b])$, suriettiva, tale che $\forall x: f'(\underbar{x}) \neq 0$, e tale che
$$\gamma(t) = \delta(f(t))$$
\paragraph{Lunghezza}
In modo simile a quanto si farebbe in Fisica, si ottiene la lunghezza di una curva integrando il modulo della sua velocità vettoriale. Essendo $\gamma$ una curva definita su $[a, b]$:
$$L= \int_a^b ||\gamma'(t)|| dt = \int_a^b\sqrt{\gamma'_1(t)^2+\gamma'_2(t)^2+...+\gamma'_n(t)^2}dt$$


\section{1-forme differenziali}
\subsubsection{Definizioni}
Una 1-forma differenziale è una funzione del tipo
$$\omega := \sum_{j=1}^n a_j(x)dx_j$$
In questo esempio, i vari $a_j(x)$ si chiamano coefficienti della 1-forma differenziale.
Una nota 1-forma differenziale è ad esempio il differenziale di una funzione.
\paragraph{Integrale di una 1-forma differenziale}
Sia $\gamma$ una curva regolare, di estremi $a$ e $b$.
Si definisce l'integrale di una 1-forma differenziale su $\gamma$ in questo modo:
$$\int_\gamma \omega:=\int_a^b \sum_{j=1}^n <a_j(\gamma(t)), \gamma'(t)> dt$$
ove l'operatore $<,>$ sia il prodotto scalare standard.
\paragraph{Forme esatte}
Una forma differenziale $\omega$, definita in un aperto $A\subset\mathbb{R}^n$ si dice esatta se è il differenziale di un'altra funzione.
Esplicitamente, la forma è esatta se esiste $f$ tale che:
$$\forall j\in\{1 ... n\}: a_j(x) = \frac{\partial f(\underbar{x})}{\partial x_j}$$
\subsubsection{Relazione tra 1-forme differenziali e campi vettoriali}

\section{Altro}
\subsection{Norme}
\paragraph{Norma 1}
La norma 1 è definita come:
$$||f||_1 := \int_a^b|f|$$
\subsection{Formule varie}
\paragraph{Esponenziale di una matrice}
Ricordiamo la definizione di esponenziale di una matrice.
$$e^{tA} := \sum_{k=0}^{+\infty} \dfrac{(tA)^k}{k!}$$
Questa scrittura ci da però pochi metodi pratici per il calcolo dell'esponenziale.

Sia allora $D$ una matrice diagonale:
$$D=\text{diag}(\lambda_1,...,\lambda_n)$$
Si può mostrare facilmente che
$$e^{tD} = \text{diag}(e^{t\lambda_1}, ..., e^{t\lambda_n})$$

Analizziamo ora il caso generale.
Sia $A$ una matrice di dimensione $n$ con un sistema completo di autovettori (colonna) $\underbar{h}_1, ..., \underbar{h}_n$ riferiti a $n$ autovalori $\lambda_1, ..., \lambda_n$. Definiamo $S$ come
$$S:=[\underbar{h}_1\ ...\ \underbar{h}_n]$$

$A$ è ora ovviamente diagonalizzabile tramite $S$:
$$S^{-1}AS=\text{diag}(\lambda_1,...,\lambda_n)$$
e
$$A = SDS^{-1}$$
Allora, con dei semplici passaggi si verifica che
$$e^{tA} = Se^{tD}S^{-1}$$
e dunque il calcolo di esponenziale di una matrice qualunque viene ricondotto al calclolo di una matrice diagonale, appena risolto.

\end{document}

