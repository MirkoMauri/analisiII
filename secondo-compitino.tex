\documentclass[a4paper,12pt]{article}
\usepackage[utf8x]{inputenc}
\usepackage{amssymb, amsmath, amsthm, mathrsfs}
\newcommand{\ubar}{\underbar}
\newcommand{\cont}{\mathscr{C}}
\newcommand{\Rint}{\mathcal{R}}
%\newtheorem{d}{Definizione}
\usepackage{fullpage}

\newtheorem{teo}{Teorema}
\newtheorem{defi}{Definizione}
\begin{document}
\begin{titlepage}
\title{Dispensa per il secondo compitino di Analisi II}
\author{Riccardo Iaconelli buuuuuuuuuuuuuuuuuuuuuuuu}
% Remove command to get current date 
\maketitle
\end{titlepage}

\begin{titlepage}
\tableofcontents
\end{titlepage}

\section{Successioni e Serie di Funzioni}
\subsection{Definizioni}
\begin{defi}
Sia $\{ f_n\} $  una successione di funzioni a valori reali o complessi definite su un medesimo $D \subseteq \mathbb{R}$.
 $\{ f_n\} $ \textbf{converge puntualmente} o semplicemente su $ E \subseteq D $ se $\forall x \in E$, $\{f_n(x)\}$ converge in $\mathbb{R}$ (o nel caso complesso $\mathbb{C}$). Chiamiamo funzione limite puntuale la funzione definita su $E$ nel modo seguente: 
 $$f(x):=\lim_{n\to+\infty} f_n(x) \qquad \qquad \forall x \in E $$

Similmente, se $\forall x \in E \subseteq D$ la serie numerica $\sum_{n=1}^{\infty} f_n(x)$ converge in $\mathbb{R}$ (o $\mathbb{C}$)  allora diremo che la serie  $\sum_{n=1}^{\infty} f_n$ converge puntualmente in E. La funzione
\begin{displaymath}
s(x):= \sum_{n=1}^{\infty} f_n(x) \qquad \qquad \forall x \in E
\end{displaymath}
si dirà somma puntuale della serie $\sum_{n=1}^{\infty} f_n$
\end{defi}


\subparagraph*{Osservazione}
Utilizzando un'altra scrittura, la definizione di convergenza puntuale diviene:
 $$\forall \varepsilon>0\qquad \forall x \in E\qquad\exists N=N(\varepsilon, x): \forall n \geq N\qquad |f_n(x) - f(x)| < \varepsilon$$ 
\begin{defi}
Sia $\{ f_n\} $ come sopra. Essa \textbf{ converge uniformente} su $E \subseteq \mathbb{R} $ alla funzione limite $f$ se:
 
$$ \lim_{n\to+\infty} \displaystyle \sup_{x \in E} |f_n\left(x\right)-f\left(x\right)| = 0$$


Similmente diciamo che la serie $\sum_{n=1}^{\infty} f_n$ converge uniformemente su $E \subseteq \mathbb{R} $ alla funzione limite $s(x)$ se e solo se

$$ \lim_{n\to+\infty}  \sup_{x \in E} \displaystyle \left|\sum_{n=1}^{\infty} f_n(x)-s(x)\right| = 0 $$

\end{defi}
\subparagraph*{Osservazione}
 
Utilizzando un'altra scrittura, la definizione di convergenza uniforme diviene:
 $$\forall \varepsilon>0\qquad \exists N=N(\varepsilon):\ \forall n \geq N\qquad |f_n(x) - f(x)| < \varepsilon\qquad \forall x\in E$$ 

\subsection{Criteri di convergenza}

\subsubsection{Successioni}
\begin{teo}[Condizione di Cauchy]
Sia $\{f_n\}$ una successione di funzioni a valori reali (o complessi). Allora $\{f_n\}$ converge uniformemente in $E$ se e solo se $\forall \varepsilon>0\ \ \exists N=N(\varepsilon):\ \forall n,m \geq N\ \forall x\in E$ si ha che 
$$|f_n(x) - f_m(x)| < \varepsilon$$.
\end{teo}
\subparagraph*{Osservazione}
La convergenza uniforme di una successione è quindi condizione \textbf{necessaria e sufficiente} per verificare il criterio di Cauchy.
\begin{teo}
Sia ${f_n}$ una successioni di funzioni definite su $K$ compatto, $f_n \in \cont(K)$; sia $f_n$ convergente puntualmente a $f$,   $f \in \cont(K)$ . Se
\begin{displaymath}
f_1 \geq f_2 \geq f_3 \geq \dots
\end{displaymath}

allora $f_n$ converge a $f$ uniformemente.
\end{teo}
\subsubsection{Serie}

\begin{teo}[Criterio di Weierstrass]
Sia $\{f_n\}$ una successione di funzioni su $E$. Sia $$M_n:= \displaystyle\sup_{x\in E}|f_n(x)|$$ e $$\sum_{n=1}^{+\infty}M_n<+\infty$$
allora $\displaystyle\sum_{n=1}^{+\infty}f_n$ converge uniformemente.
\end{teo}
\subsection{Convergenza Uniforme e Continuità}
\begin{teo}[Successioni di funzioni continue]
Sia $\{f_n\}$ una successione di funzioni continue definite in $E$ uniformemente convergenti a $f$. Allora $f$ è continua.
\end{teo}
\subsection{Convergenza Uniforme e Integrabilità}
\begin{teo}
Sia ${f_n}$ una successione di funzioni R-integrabili in $[a,b]\subseteq \mathbb{R}$, uniformemente convergente a $f$ in $[a,b]$. Allora $f\in \Rint([a,b])$ e
$$\lim_{n\to\infty}\int_{a}^{b} f_n=\int_{a}^{b}f$$
\end{teo}
\subsection{Convergenza Uniforme e Differenziabilità}
\begin{teo}
Sia $f_n$ una successione di funzioni differenziabili in  $[a,b]\subseteq \mathbb{R}$. Supponiamo che esista un $x_{0} \in [a,b] $ per il quale la successione numerica ${f_n(x_0)}$ converga e che ${f^{'}(x)}$ converga uniformemente in $[a,b]$. Allora ${f_n}$ converge uniformemente in $[a,b]$ a una funzione $f$ e
$$\lim_{n\to\infty} f^{'}_n (x)=f^{'}(x) \qquad \qquad \forall x \in [1,b]$$
\end{teo}

\subsection{Come risolvere gli esercizi}
\subsubsection{Successioni}
\paragraph{Punto 0}
Cerca dominio, etc...
\paragraph{Convergenza puntuale}
Calcola limite...
\paragraph{Convergenza uniforme}
Sup di fn...

\subsubsection{Serie}


\subsection{Contrazioni in spazi metrici}
\subsubsection{Definizioni}
Sia $(X, d)$ uno spazio metrico, sia $\phi : X\rightarrow X$.
\paragraph{Contrazione}
Si dice che $\phi$ è una contrazione di $X$ in sé quando:
$$\exists c < 1:\ \ d(\phi(x_1), \phi(x_2)) \leq cd(x_1, x_2)\qquad\qquad\forall x_1, x_2 \in X$$
\paragraph{Punto fisso}
Si dice che $\bar{x}\in X$ è punto fisso per $\phi$ se:
$$\phi(\bar{x})=\bar{x}$$

\subsection{Lipschitzianità}
Sia $f:\Omega\rightarrow\mathbb{R}$ ove $\Omega\subset\mathbb{R}^n$, aperto.
\paragraph{Lipschitzianità semplice}
Si dice $f\left(x\right)$ lipschitziana se $\exists L>0$ t.c
$$ |f\left(x\right)- f\left(y\right)| \leq L|x-y|\qquad\qquad\forall x,y \in \Omega $$

\paragraph{Lipschitzianità uniforme}
Si dice che $f(t, v)$ è lipschitziana rispetto a $v$, uniformemente rispetto a $t$, su $\Omega$ se $\exists L>0$ t.c.
$$|f(t, v) - f(t, w)| \leq L|v-w|\qquad\qquad\forall (t,v), (t, w) \in \Omega$$
In altre parole, fissata la variabile $t$, $f_t(v)$ è lipschitziana.

\paragraph{Lipschitzianità locale}
Si dice che $f(t, v)$ è localmente lipschitziana rispetto a $v$, uniformemente rispetto a $t$, se
$$\exists U((t, v), r) : f(t, v)|_U \in \text{Lip}_t(v)\qquad\qquad\forall(t, v)\in \Omega$$
(esiste un intorno $U$ di $(t, v)$ tale che la funzione ristretta a quell'intorno sia lipschitziana rispetto a $v$, uniformemente rispetto a $t$).

\subsubsection{Teoremi}
\paragraph{Teorema di Banach-Caccioppoli}
Sia $(X, d)$ uno spazio metrico, sia $\phi : X\rightarrow X$ una contrazione.
Allora $\phi$ ha un unico punto fisso.

\section{Serie di potenze}
\subsection{Definizioni}
Una serie di potenze è una serie di funzioni della forma
$$\label{seriepotenza} \displaystyle \sum_{n=0}^\infty c_n\left(x-x_0\right)^n$$
con $x_0 \in \mathbb{R}$ fissato, ${c_n}$ è successione di numeri reali e $x$ una variabile reale. 
E' sempre possibile ricondursi al caso $x_0 = 0$ con una opportuna traslazione. L'insieme di convergenza di \ref{seriepotenza} è l'insieme degli $x \in \mathbb{R}$ tali che \ref{seriepotenza} converge. L'insieme di convergenza di una serie di potenze è sempre un \textbf{intervallo.}
\paragraph{Teorema della convergenza semplice}
Data la serie di potenze 
$$ \displaystyle \sum_{n=0}^\infty c_n\left(x\right)^n$$
poniamo $$ \alpha:=\limsup_{n\to \infty} \sqrt[n]{|c_n x^n|} \ e \ R:=\frac{1}{\alpha}$$
(se $\alpha = +\infty$, allora $R=0$ e se $\alpha=0$, allora $R=+\infty$)
La serie di potenze converge se $|x|<R$ e non converge se $|x|>R$.
R è il raggio di convergenza della serie.
\paragraph{Teorema della convergenza uniforme delle serie di potenza}
Supponiamo che $ \displaystyle \sum_{n=0}^\infty c_n\left(x\right)^n$ converga per $|x|<r$. Poniamo:
$$ f(x):= \displaystyle \sum_{n=0}^\infty c_n\left(x\right)^n \ \ \forall t \in (-r,r) \forall \varepsilon > 0 $$
Allora $$ \displaystyle \sum_{n=0}^\infty c_n\left(x\right)^n$$ converge uniformente a $f(x)$ in $[-r+\varepsilon,r-\varepsilon]$ Inoltre: $f \in \cont^\infty((-r,r))$ $$f^{(k)}(x) = \displaystyle \sum_{n=0}^\infty n(n-1)\dots(n-k+1)c_n x^{n-k} \ \ \forall x \in (-r,r)$$  $$f^{(k)}(0) = k!\ c_k \ \ k=0,1,2,3 \dots $$ 

\paragraph{Serie di Taylor}
La serie:
$\displaystyle \sum_{n=0}^\infty \frac{f^{n}(0)}{n!} x^n$ si chiama serie di Taylor di $f$. Condizione necessaria affinchè la serie di Taylor di $f$ converga in  $x$ è che $$\lim_{n \to \infty}\displaystyle \left[ f(x) - \displaystyle  \sum_{k=0}^\infty \frac{f^{k}(0)}{k!}x^k \right] = 0 $$
\subsection{Esercizi}

\section{Equazioni differenziali}

\subsection{Definizioni}
\paragraph{Il problema di Cauchy}
Siano $f:\Omega \subseteq \mathbb{R}^2 \longrightarrow $ e $ \left(t_0,x_0\right) \in \Omega.$ 
$$\label{PC}\begin{cases} x'=f\left(t,x\right) \\ x\left(t_0\right)=x_0 \end{cases}$$

Una soluzione del problema di Cauchy è una coppia $\left( \phi , I \right)$ costituita da una funzione $\phi: I \longrightarrow \mathbb{R}$ differenziabile in $I$ tale che:
\begin{itemize}
\item $t_0 \in I$ e $ \phi\left(t_0\right)=x_0$
\item $ \left(t,\phi\left(t\right)\right) \in \Omega \ \forall t \in I$
\item $ \phi^{'}\left(t \right) = f\left(t,\phi\left(t\right)\right)\ \forall t \in I$
\end{itemize}

Se $\left(\phi,I\right)$ risolve il problema di Cauchy, allora è una soluzione continua dell'equazione di Volterra:
$$ x\left(t\right) = x_0 + \displaystyle\int^t_{t_0} f\left(s,x\left(s\right)\right) ds $$

\paragraph{Teorema di esistenza e unicità in piccolo o locale}
Se $f \in \cont \left(\Omega\right)$ allora PC ha almeno una soluzione. \\
Se $f \in \cont \left(\Omega\right) \cap \text{Lip}^{\text{loc}}_x\left(\Omega\right)$, allora PC ha un'unica soluzione in un opportuno intorno di $t_0$.\\
Una funzione di classe $\cont^n$ con $ n \geq 1 $ è localmente lipschitziana. 

\paragraph{Teorema di esistenza e unicità in grande o globale}
Siano $-\infty < \tau_1 < \tau_2 < +\infty$ e $\S:=\left[\tau_1,\tau_2\right]\times\mathbb{R}$. Sia $f \in C\left(\Omega\right) \cap \text{Lip}^{\text{loc}}_x\left(\Omega\right)$ con crescita al più lineare in x, cioè
$$\exists a,b: |f \left(t,x\right)|\leq a + b|x|$$
allora l'unica soluzione di 
$$\begin{cases} x'=f\left(t,x\right) \\ x\left(t_0\right)=x_0 \end{cases}$$
dove $\left(t_0,x_0\right)$ è un punto qualunque in S, è definita in $\left[\tau_1,\tau_2\right]$.

\paragraph{Teorema di prolungamento}
Siano  $f \in C\left(\Omega\right) \cap \text{Lip}^{\text{loc}}_x\left(\Omega\right)$ e $\left(t_0,x_0\right) \in \Omega$. Siano $\left(\phi,I\right)$ la soluzione massimale di PC e $K$ cpt $\subset \Omega$. Allora il grafico di $\phi$ esce definitivamente da $K$ per $t\rightarrow t^+_{min}$ e $t\rightarrow t^-_{max}$, con $I:=(t_{min},t_{max})$.



\subsection{Sistemi lineari omogenei}
Un sistema lineare omogeneo (LO) di equazioni differenziali è un sistema della forma:
$$\underbar{x}' = A(t)\underbar{x}$$ 
ove $$A(t)\in M_n(\mathbb{R})\qquad \qquad \forall t\in I\subset R$$ e $\underbar{x} = (x_1,x_2,x_3 \dots x_n)^t$
\paragraph{Osservazioni}
\begin{itemize}
 \item Lo spazio delle soluzioni di LO è uno spazio vettoriale di dimensione $n$.
 \item Il PC associato a LO ha una e una sola soluzione definita su tutto $I$.
 \item $A(t)\underbar{x}$ ha crescita al più lineare.
 \item Se $\phi_1,...,\phi_n$ sono soluzioni di LO, queste sono linearmente indipendenti se e solo se $w(t)\neq0$ $\forall t \in I$.
\end{itemize}


\paragraph{Matrice wronskiana}
Siano $\phi_1,...,\phi_n$ funzioni a valori su $\mathbb{R}^n$ definite in $I\subset \mathbb{R}$.
Si chiama \textbf{matrice wronskiana} di $\phi_1,...,\phi_n$ la matrice:
$$W(t):=[\phi_1(t)\ ...\ \phi_n(t)]$$
Si chiama altresì \textbf{wronskiano} di $\phi_1,...,\phi_n$:
$$w(t):=\text{det } W(t)$$
\subparagraph*{Osservazione} l'annullarsi del wronskiano è condizione necessaria (ma non sufficiente) per la dipendenza lineare di $\phi_1,...,\phi_n$. 
Condizione necessaria e sufficiente affinchè $n$ soluzioni su $I$ di una medesima equazione lineare omogenea siano linearmente indipendenti è che:
$$ w(t)\neq0 \qquad\qquad \forall t \in I$$

\subsection{Sistemi lineari non omogenei}
Un sistema lineare non omogeneo (LNO) di equazioni differenziali è un sistema della forma:
$$\underbar{x}' = A(t)\underbar{x} + \underbar{b}(t)$$
\paragraph{Osservazioni}
\begin{itemize}
 \item Lo spazio delle soluzioni di (LNO) non è uno spazio vettoriale, se $\underbar{b}\neq\underbar{0}$.
 \item Sia $\Psi^*$ un soluzione di (LNO). Tutte e sole le soluzioni di (LNO) sono allora del tipo
       $$ \Psi(t) = \phi(t) + \Psi^*(t)\qquad \qquad \forall t \in I$$
       dove $\phi$ è una soluzione qualunque dell'(LO) associato ($\underbar{b}=\underbar{0}$).
 \item Se $A(t) , \underbar{b}\in \cont(I)$, allora $ \forall t_0 \in I$ il (PC)  ha una e una sola soluzione.
 \item Se $\phi_1,...,\phi_n$ sono soluzioni di (LO), queste sono linearmente indipendenti se e solo se $w(t)\neq0$ $\forall t \in I$.
\end{itemize}


\paragraph{Integrale generale}
L'integrale generale è una soluzione di una equazione differenziale.\\
Ad esempio, l'integrale generale, o soluzione generale, di un'equazione differenziale lineare non omogenea è data dalla somma tra una funzione soluzione dell'omogenea associata e una soluzione dell'equazione non omogenea. La soluzione particolare dell'equazione non omogenea è anche detta \textbf{integrale particolare}. Parlare di integrare un'equazione differenziale è equivalente a parlare di risolvere un'equazione differenziale.

\subsection{Sistemi lineari omogeni a coefficienti costanti}:

$$\underbar{x}' = A\underbar{x}$$
dove $A \in M_n(\mathbb{R})$. Possiamo esprimere le soluzioni di questo sistema mediante l'esponenziale di $tA$.
L'esponenziale gode delle seguenti proprietà:
\begin{itemize}
\item $e^{(s+t)A} = e^{sA}e^{tA} \ \ \forall s,t \in \mathbb{R} $
\item 
\end{itemize}}

\subsection{Sistemi lineari non omogenei a coefficienti costanti}
Un sistema lineare non omogeneo (a coefficienti costanti) di equazioni differenziali è un sistema del tipo
$$\underbar{x}'=A\underbar{x}+\underbar{b}(t)$$
Ove $A\in M_n(\mathbb{R})$ e $b$ continua, allora, per ogni punto $t_0 \in \mathbb{R}$:
$$\Psi^*(t)= \int_{t_0}^t e^{(t-s)A} \underbar{b}(s)ds$$
è una soluzione particolare dell'equazione sopra.
Per orttenere una soluzione più specifica, ad esempio ponendo una condizione iniziale quale
$$\underbar{x}(t_0)=\underbar{x}_0$$
Allora si ha che
$$\phi(t)=e^{(t-t_0)A}\underbar{x}_0+\Psi^*(t)$$
ovvero si somma alla soluzione generale del sistema omogeneo associato la soluzione particolare trovata in precedenza.
Vedi anche il metodo di variazione delle costanti arbitrarie.

\subsection{Risolvere un esercizio}
In generale, risolvere (integrare) un'equazione differenziale consiste principalmente nel riconoscere un tipo di equazione, ed applicare procedimenti noti. Vediamone qualcuno.

\subsubsection{Metodo di variazione delle costanti arbitrarie}
Questo metodo (alternativo, e un pochino più semplice) viene utilizzato se conosciamo un sistema fondamentale di soluzioni di un (LO), e cerchiamo almeno una soluzione di un (LNO).
Siano $\phi_1,...,\phi_n$ soluzioni di un (LO) $y'= A(t)y$, e sia $W(t)$ la corrispondente matrice wronskiana.
Ricerchiamo ora una soluzione di (LNO) della forma
$$\Psi^*(t)=W(t)\underbar{c}(t)$$
ove le nostre incognite sono le varie componenti della funzione $\underbar{c}(t)$
Ricordando che vale la formula:
$$\frac{dW(t)}{dt} = A(t)W(t)$$
(per verifica diretta) si trova che, chiamando $V(t) = W^-1(t)$, e imponendo che $$\underbar{c}'(t) = V(t)\underbar{b}(t)$$
si trova l'integrale particolare cercato:
$$\Psi^*(t)= W(t)\int_{t_0}^t V(t)\underbar{b}(s) ds$$

\subsection{Equazioni lineari}
Con un'equazione di forma:
$$y'(t) = P(t)y+Q(t)$$
Allora
$$y(t)=e^{- \displaystyle\int P(t)dt}\left(k + \int Q(t)e^{-\displaystyle\int P(t)dt}dt\right)$$
\subsection{Equazione di Bernoulli}
Con un'equazione di forma:
$$y'(t) = P(t)y+Q(t)$$
con $\alpha \neq 0, 1$. Allora, si impone:
$$ z(t)=y(t)^{1-\alpha} $$
si risolve l'equazione lineare in $z$: $$z'(t)=(1-\alpha)P(t)z(t)+Q(t)$$ e, trovata $z(t)$ con la formula sopra, si ricava $y(t)$.

\subsection{Equazione di Riccati}
$$y'(t) = P(t)y^2+Q(t)y+R(t)$$
Conoscendo una soluzione particolare $\Psi(t)$, allora applico la sostituzione
$$y(t)=\Psi(t)+\dfrac{1}{z(t)}$$
e con un po' di passaggi si dimostra che alla fine
$$z'(t)= -\left[2\Psi(t)P(t)+Q(t)\right]z(t)-P(t)$$
che è lineare.
\subsection{Variabili separabili}
Se invece
$$y'(t) = f(t)g(y)$$
Allora ovviamente si risolve ricordando che
$$\int\dfrac{dy}{g(y)}=\int f(t)dt + c$$
\subsection{Un'altro tipo di equazioni omogenee}
$$y'(t) = f\left(\dfrac{y(t)}{t}\right)$$
Allora si sostituisce
$$z=\dfrac{y}{t}$$
da cui $$z'(t) = \dfrac{dz}{dt} = \dfrac{f(z)-z}{t}$$
e dunque
$$\int\dfrac{dz}{f(z)-z}=\int \dfrac{dt}{t} + c$$
sicché alla fine:
$$\int\dfrac{dz}{f(z)-z} = \log|t| + c$$

Risostituendo e risolvendo rispetto a $y$ si ottiene la soluzione cercata.

\subsection{Equazioni lineari di ordine $n$}
Sia
$$y^{(n)} + a_1(t)y^{(n-1)} + a_2y^{(n-2)} + ... + a_n(t)y = 0$$
ove le funzioni $a_j \in C(I)$
Allora il sistema è equivalente al sistema:
$$\underbar{Y}' = A(t)\underbar{Y}$$
ove $\underbar{Y}=(y, y', y^{(2)}, ..., y^{(n)})$ (ogni componente è trattata come componente a sé stante, e non esplicitamente come derivata) e
$$A(t) =
\begin{bmatrix}
0 & 1 & 0 & ... & 0\\
0 & 0 & 1 & ... & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & ... & 1 \\
-a_n(t) & -a_{n-1}(t) & -a_{n-2}(t) & ... & -a_{1}(t) \\
\end{bmatrix}
$$

che si risolve come un (LO) qualunque.

\subsubsection{Oscillatori armonici}
\paragraph{Oscillazioni libere}
Data un'equazione di forma
$$x''+\omega^2 x=0$$
la soluzione è del tipo
$$\psi(t) = c_1 e^{i\omega t} + c_2 e^{-i\omega t}$$
oppure
$$\psi(t) = c'_1\sin(\omega t) + c'_2\cos(\omega t)$$
a seconda della base scelta.

\paragraph{Risonanza??}
Invece, data un'equazione di forma
$$x''-\omega^2 x=0$$
questa ha soluzioni reali, più precisamente del tipo
$$\psi(t) = c_1 e^{\omega t} + c_2 e^{-\omega t}$$

\paragraph{Oscillazioni smorzate}
Nel terzo caso, abbiamo un'equazione del tipo
$$x''-2x +x=0$$
la cui soluzione è del tipo
$$\phi(t) = c_1e^t +c_2 t + e^t$$
(check)

\paragraph{Casi non omogenei}
In un caso del tipo
$$x''+\omega^2 x=b(t)$$
bisogna sommare una soluzione particolare. Si scopre facilmente che tutte e sole le soluzioni sono fatte
$$\phi(t) = c'_1\sin(\omega t) + c'_2\cos(\omega t) + \frac{1}{\omega^2}t$$

o equivalentemente scegliendo una base complessa, la soluzione particolare da sommare è $\psi^*(t) = \frac{1}{\omega^2}t$.

\section{Curve in $\mathbb{R}^n$}
\subsection{Definizioni principali}
\paragraph{Curva, sostegno}
Una curva in $\mathbb{R}^n$ è una funzione continua $\gamma:[a,b]\rightarrow\mathbb{R}^n$.
Il suo sostegno è l'immagine di $[a,b]$ tramite $\gamma$. Viene anche detto ``parametrizzazione''.
\paragraph{Velocità vettoriale}
Sia $\gamma$ una curva in $\mathbb{R}^n$. Allora la velocità vettoriale $\underbar{v}(t)$ è definita come
$$\underbar{v}(t) := \lim_{t\to t_0} \dfrac{\gamma(t)-\gamma(t_0)}{t-t_0}$$

\paragraph{Regolarità}
Una curva $\gamma:[a,b]\rightarrow\mathbb{R}^n$ si dice regolare se è di classe $C^1$. Si dice invece regolare a tratti se esiste una partizione $\{x_j\}$ di $[a,b]$ tale che $\gamma|_{[x_j, x_{j+1}]}$ è regolare.
\paragraph{Equivalenza}
Due curve regolari $\gamma :[a,b]\rightarrow\mathbb{R}^n$ e $\delta :[c,d]\rightarrow\mathbb{R}^n$ si dicono equivalenti se esiste una funzione 
$f:[a,b]\rightarrow[c,d], f \in C^1([a,b])$, suriettiva, tale che $\forall x: f'(\underbar{x}) \neq 0$, e tale che
$$\gamma(t) = \delta(f(t))$$
\paragraph{Lunghezza}
In modo simile a quanto si farebbe in Fisica, si ottiene la lunghezza di una curva integrando il modulo della sua velocità vettoriale. Essendo $\gamma$ una curva definita su $[a, b]$:
$$L= \int_a^b ||\gamma'(t)|| dt = \int_a^b\sqrt{\gamma'_1(t)^2+\gamma'_2(t)^2+...+\gamma'_n(t)^2}dt$$


\section{1-forme differenziali}
\subsubsection{Definizioni}
Una 1-forma differenziale è una funzione del tipo
$$\omega := \sum_{j=1}^n a_j(x)dx_j$$
In questo esempio, i vari $a_j(x)$ si chiamano coefficienti della 1-forma differenziale.
Una nota 1-forma differenziale è ad esempio il differenziale di una funzione.
\paragraph{Integrale di una 1-forma differenziale}
Sia $\gamma$ una curva regolare, di estremi $a$ e $b$.
Si definisce l'integrale di una 1-forma differenziale su $\gamma$ in questo modo:
$$\int_\gamma \omega:=\int_a^b \sum_{j=1}^n <a_j(\gamma(t)), \gamma'(t)> dt$$
ove l'operatore $<,>$ sia il prodotto scalare standard.
\paragraph{Forme esatte}
Una forma differenziale $\omega$, definita in un aperto $A\subset\mathbb{R}^n$ si dice esatta se è il differenziale di un'altra funzione.
Esplicitamente, la forma è esatta se esiste $f$ tale che:
$$\forall j\in\{1 ... n\}: a_j(x) = \frac{\partial f(\underbar{x})}{\partial x_j}$$
\subsubsection{Relazione tra 1-forme differenziali e campi vettoriali}

\section{Altro}
\subsection{Norme}
\paragraph{Norma 1}
La norma 1 è definita come:
$$||f||_1 := \int_a^b|f|$$
\subsection{Formule varie}
\paragraph{Esponenziale di una matrice}
Ricordiamo la definizione di esponenziale di una matrice.
$$e^{tA} := \sum_{k=0}^{+\infty} \dfrac{(tA)^k}{k!}$$
Questa scrittura ci da però pochi metodi pratici per il calcolo dell'esponenziale.

Sia allora $D$ una matrice diagonale:
$$D=\text{diag}(\lambda_1,...,\lambda_n)$$
Si può mostrare facilmente che
$$e^{tD} = \text{diag}(e^{t\lambda_1}, ..., e^{t\lambda_n})$$

Analizziamo ora il caso generale.
Sia $A$ una matrice di dimensione $n$ con un sistema completo di autovettori (colonna) $\underbar{h}_1, ..., \underbar{h}_n$ riferiti a $n$ autovalori $\lambda_1, ..., \lambda_n$. Definiamo $S$ come
$$S:=[\underbar{h}_1\ ...\ \underbar{h}_n]$$

$A$ è ora ovviamente diagonalizzabile tramite $S$:
$$S^{-1}AS=\text{diag}(\lambda_1,...,\lambda_n)$$
e
$$A = SDS^{-1}$$
Allora, con dei semplici passaggi si verifica che
$$e^{tA} = Se^{tD}S^{-1}$$
e dunque il calcolo di esponenziale di una matrice qualunque viene ricondotto al calclolo di una matrice diagonale, appena risolto.

\end{document}

